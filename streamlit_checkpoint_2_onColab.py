# -*- coding: utf-8 -*-
"""Streamlit checkpoint 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pMq2B_eQNPEKxMteS4WdwzvZ6sksMEuD
"""

import pandas as pd

df = pd.read_csv('/content/Financial_inclusion_dataset.csv')

df.head()

df.isnull().sum()

df.drop(columns=['uniqueid','year'], inplace=True)

df.shape

df.describe()

!pip install ydata-profiling
from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Expresso Churn Report", explorative=True)

profile.to_file("expresso_churn_report.html")

profile.to_notebook_iframe()

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['country_encoded'] = le.fit_transform(df['country'])
le1 = LabelEncoder()
df['bank_account_encoded'] = le1.fit_transform(df['bank_account'])
le2 = LabelEncoder()
df['location_type_encoded'] = le2.fit_transform(df['location_type'])
le3 = LabelEncoder()
df['cellphone_access_encoded'] = le3.fit_transform(df['cellphone_access'])
le4 = LabelEncoder()
df['gender_of_respondent_encoded'] = le4.fit_transform(df['gender_of_respondent'])
le5 = LabelEncoder()
df['relationship_with_head_encoded'] = le5.fit_transform(df['relationship_with_head'])
le6 = LabelEncoder()
df['marital_status_encoded'] = le6.fit_transform(df['marital_status'])
le7 = LabelEncoder()
df['education_level_encoded'] = le7.fit_transform(df['education_level'])
le8 = LabelEncoder()
df['job_type_encoded'] = le8.fit_transform(df['job_type'])

df1= df.drop(columns=['country', 'bank_account', 'location_type', 'cellphone_access', 'gender_of_respondent', 'relationship_with_head', 'marital_status','education_level', 'job_type'])

df1.info()

df1.isnull().sum().sum()

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 6))
sns.boxplot(data=df1)
plt.xticks(rotation=90)
plt.title('Boxplots of the data')
plt.show()

Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1

# Filter out outliers
df1 = df1[~((df1 < (Q1 - 1.5 * IQR)) | (df1 > (Q3 + 1.5 * IQR))).any(axis=1)]

len(df1.columns)

df1.columns

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Check column names in the DataFrame
print(df1.columns)  # or print(df.columns) if that's the DataFrame being used

# List of columns to plot
columns = [
    'household_size', 'age_of_respondent', 'country_encoded',
       'bank_account_encoded', 'location_type_encoded',
       'cellphone_access_encoded', 'gender_of_respondent_encoded',
       'relationship_with_head_encoded', 'marital_status_encoded',
       'education_level_encoded', 'job_type_encoded'
]

# Set up a plotting grid
n_cols = 3
n_rows = (len(columns) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))

# Flatten axes for easy iteration
axes = axes.flatten()

# Plotting loop for histograms and bar plots
for idx, col in enumerate(columns):
    ax = axes[idx]
    try:
        if df1[col].dtype in ['int64', 'float64']:
            # If numeric, plot histogram
            ax.hist(df1[col].dropna(), bins=20, color='red', alpha=0.7)
            ax.set_title(f'Histogram of {col}')
            ax.set_xlabel(col)
            ax.set_ylabel('Frequency')
        else:
            # If categorical, plot bar plot
            value_counts = df1[col].value_counts()
            ax.bar(value_counts.index.astype(str), value_counts.values, color='red', alpha=0.7)
            ax.set_title(f'Bar Plot of {col}')
            ax.set_xlabel(col)
            ax.set_ylabel('Count')
            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    except KeyError:
        print(f"Column {col} not found in the DataFrame")

# Remove empty subplots if any
for j in range(idx + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

df1.describe()

from sklearn.preprocessing import StandardScaler

cols_to_scale = ['household_size',	'age_of_respondent']

scaler = StandardScaler()
df1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score, confusion_matrix

# Split the dataset into features (X) and target (y)
X = df1.drop('bank_account_encoded', axis=1)
y = df1['bank_account_encoded']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)

# Train a Random Forest Classifier
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

import joblib

model = RandomForestRegressor()
# Save the trained model to a file
joblib.dump(model, 'random_forest_model.pkl')

import pickle

with open ('le.pkl', 'wb') as file:
  pickle.dump(le, file)

with open ('le1.pkl', 'wb') as file:
  pickle.dump(le1, file)

with open ('le2.pkl', 'wb') as file:
  pickle.dump(le2, file)

with open ('le3.pkl', 'wb') as file:
  pickle.dump(le3, file)

with open ('le4.pkl', 'wb') as file:
  pickle.dump(le4, file)

with open ('le5.pkl', 'wb') as file:
  pickle.dump(le5, file)

with open ('le6.pkl', 'wb') as file:
  pickle.dump(le6, file)

with open ('le7.pkl', 'wb') as file:
  pickle.dump(le7, file)

with open ('le8.pkl', 'wb') as file:
  pickle.dump(le8, file)

joblib.dump(scaler, 'scaler.pkl')

scaler = joblib.load('scaler.pkl')

df1.columns